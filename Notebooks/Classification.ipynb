{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97cd39a-4aec-4f8c-b86d-651726cc1410",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x13e07600f390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathlib import Path\n",
    "from pylab import savefig\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a24fce0-f6bf-4592-8308-1fa255596b8e",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "def prepare_data(path):\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize([224,224]),\n",
    "          #  transforms.Resize([480,480]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize([224,224]),\n",
    "        #transforms.CenterCrop(224),\n",
    "           # transforms.Resize([480,480]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    data_dir = path\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                              data_transforms[x])\n",
    "                      for x in ['train', 'test']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
    "                                                 shuffle=True, num_workers=4)\n",
    "                  for x in ['train', 'test']}\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    #print(class_names)\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return dataloaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7167e4a-30a6-420d-a302-10bd85f2c7e9",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, fold_num):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"train model girildi\")\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'test']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'test' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "        \n",
    "    \n",
    "    print(\"train model çıkıldı\")\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f391c226-4c33-4031-a67e-c7375328de52",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=4):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//3, 3, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119318b-77f9-466e-b8d3-3f1911d11920",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conf_mat(model,fold_idx):\n",
    "    class_names = [\"Normal\", \"Tapered\",\"Pyriform\", \"Amorphous\"]\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"confmat girildi\")\n",
    "    fold_loss_values = []\n",
    "    fold_correct = 0\n",
    "    fold_total = 0\n",
    "    fold_true = []\n",
    "    fold_predicted = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['test']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            fold_total += labels.size(0)\n",
    "            fold_correct += (predicted == labels).sum().item()\n",
    "            fold_true.extend(labels.cpu().numpy())\n",
    "            fold_predicted.extend(predicted.cpu().numpy())\n",
    "                \n",
    "    return(fold_total,fold_correct,fold_true,fold_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d37cd2-606f-4f76-8405-e45fe0d31c77",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def restart_model(opti,learning_rate_selection):\n",
    "    print(\"restart model was started\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #print(device)\n",
    "    #model_ft = models.efficientnet_v2_m(weights='IMAGENET1K_V1')\n",
    "    #model_ft.name = 'efficientnet_v2_m'\n",
    "    #model_ft = models.efficientnet_v2_s(weights='IMAGENET1K_V1')\n",
    "    #model_ft.name = 'efficientnet_v2_s'\n",
    "    model_ft = models.densenet201(weights='IMAGENET1K_V1')\n",
    "    model_ft.name = 'Densenet201'\n",
    "    #print(model_ft)\n",
    "    #num_ftrs = model_ft.AuxLogits.fc.in_features # inception\n",
    "    #model_ft.aux_logits=False\n",
    "    \n",
    "    #model_ft.AuxLogits.fc = nn.Linear(num_ftrs, 18) #inception\n",
    "    \n",
    "    #num_ftrs = model_ft.fc.in_features #ResNet\n",
    "    num_ftrs = model_ft.classifier.in_features # DenseNet\n",
    "    #num_ftrs = model_ft.classifier[1].in_features\n",
    "    #num_ftrs = model_ft.heads[0].in_features\n",
    "    #num_ftrs=model_ft.head.in_features        \n",
    "        \n",
    "    #model_ft.classifier[1] = nn.Linear(num_ftrs, 3)   #ConvNext için Dene\n",
    "    #model_ft.head = nn.Linear(num_ftrs, 18) \n",
    "\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 4)\n",
    "    #print(model_ft)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "    if opti== 'Adamax':\n",
    "        optimizer_ft = optim.Adamax(model_ft.parameters(), lr=learning_rate_selection)\n",
    "    elif opti == 'SGD':\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=learning_rate_selection, momentum=0.9)\n",
    "    elif opti == 'RMSprop':\n",
    "        optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=learning_rate_selection, momentum=0.9)\n",
    "\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)\n",
    "    return(model_ft,criterion,optimizer_ft,exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4662893-458e-4b83-abeb-f9aa5042f8fc",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def genel_fold_basarisi(fold_total,fold_correct,fold_true,fold_predicted):\n",
    "    print(\"Fold accuracy was written\")\n",
    "    fold_accuracy = (fold_correct / fold_total) * 100\n",
    "    fold_accuracies.append(fold_accuracy)\n",
    "    fold_true_labels.extend(fold_true)\n",
    "    fold_predicted_labels.extend(fold_predicted)\n",
    "    return(fold_accuracies,fold_true_labels,fold_predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de788f-cac1-4e34-a372-0cb46f98ea24",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fold_conf_mat(fold_true_labels,fold_predicted_labels,opti,learning_rate_selection,imageName,txtName,model_ft):\n",
    "    print(\"fold conf mat girildi\")\n",
    "    class_names = [\"Normal\", \"Tapered\",\"Pyriform\", \"Amorphous\"] \n",
    "    \n",
    "    \n",
    "    confusion_mtx = metrics.confusion_matrix(fold_true_labels, fold_predicted_labels)\n",
    "    class_to_label = {'Normal': 0, 'Tapered': 1,'Pyriform':2, 'Amorphous': 3}\n",
    "\n",
    "    \n",
    "    df_cm = pd.DataFrame(confusion_mtx, index = [i for i in class_to_label],\n",
    "                columns = [i for i in class_to_label])\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.title(f'General Confusion Matrix for {model_ft.name} - {opti} - {learning_rate_selection}')\n",
    "    result_confmat= sn.heatmap(df_cm, annot=True,cmap=\"OrRd\",fmt=\"d\",annot_kws={\"size\": 11},cbar=False)\n",
    "    plt.ylabel('True labels',fontsize=12)\n",
    "    plt.xlabel('Predicted labels',fontsize=12)\n",
    "    figure = result_confmat.get_figure()    \n",
    "    figure.savefig(imageName, dpi=400)\n",
    "    g_fold_class_report = classification_report(fold_true_labels, fold_predicted_labels, target_names=class_names, digits=4)\n",
    "    print(f\"Classification Report for Dataset for {model_ft.name} - {opti} - {learning_rate_selection}:\\n{g_fold_class_report}\")\n",
    "    f=open(txtName,\"w\")\n",
    "    f.write(g_fold_class_report)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c8872",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dataset_path = '<Dataset Path>'\n",
    "output_path = '<Output Path>'\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55653ec4-65b2-4d02-8dc8-c839b0a7636c",
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "for opti in ['Adamax','RMSprop',  'SGD']:\n",
    "    if opti== 'Adamax':\n",
    "        Dizi = [0.001]\n",
    "    elif opti == 'RMSprop':\n",
    "        break\n",
    "        Dizi = [0.0001]\n",
    "    elif opti == 'SGD':\n",
    "        break\n",
    "        Dizi = [0.001]\n",
    "\n",
    "    for learning_rate_selection in Dizi:\n",
    "        fold_accuracies = []\n",
    "        fold_true_labels = []\n",
    "        fold_predicted_labels = []\n",
    "        print(f'Optimizer: {opti}')\n",
    "        print(f'Learning Rate: {learning_rate_selection}')\n",
    "\n",
    "        dataset_dir = dataset_path + '/fold_1'\n",
    "        dataloaders, dataset_sizes = prepare_data(dataset_dir)\n",
    "        model_ft,criterion,optimizer_ft,exp_lr_scheduler=restart_model(opti,learning_rate_selection)\n",
    "\n",
    "        imageName = output_path+'/'+ Path(dataset_path).parts[-1] +'_'+ model_ft.name + '_' + optimizer_ft.__class__.__name__ + '_' + str(learning_rate_selection) + '_Epoch_' + str(num_epochs) + '.png'\n",
    "        txtName = output_path+'/'+ Path(dataset_path).parts[-1] +'_'+ model_ft.name + '_' + optimizer_ft.__class__.__name__ + '_' + str(learning_rate_selection) + '_Epoch_' + str(num_epochs)+'_Metrics.txt'\n",
    "        \n",
    "        model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs,fold_num=1)\n",
    "        fold_total,fold_correct,fold_true,fold_predicted = conf_mat(model_ft,\"1\")\n",
    "        fold_accuracies,fold_true_labels,fold_predicted_labels = genel_fold_basarisi(fold_total,fold_correct,fold_true,fold_predicted)\n",
    "        print(\"fold 1 analizi bitti\")\n",
    "\n",
    "        dataset_dir = dataset_path + '/fold_2'\n",
    "        dataloaders, dataset_sizes = prepare_data(dataset_dir)\n",
    "        model_ft,criterion,optimizer_ft,exp_lr_scheduler=restart_model(opti,learning_rate_selection)\n",
    "        model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs,fold_num=2)\n",
    "        fold_total,fold_correct,fold_true,fold_predicted = conf_mat(model_ft,\"2\")\n",
    "        fold_accuracies,fold_true_labels,fold_predicted_labels = genel_fold_basarisi(fold_total,fold_correct,fold_true,fold_predicted)\n",
    "        print(\"fold 2 analizi bitti\")\n",
    "\n",
    "        dataset_dir = dataset_path + '/fold_3'\n",
    "        dataloaders, dataset_sizes = prepare_data(dataset_dir)\n",
    "        model_ft,criterion,optimizer_ft,exp_lr_scheduler=restart_model(opti,learning_rate_selection)\n",
    "        model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs,fold_num=3)\n",
    "        fold_total,fold_correct,fold_true,fold_predicted = conf_mat(model_ft,\"3\")\n",
    "        fold_accuracies,fold_true_labels,fold_predicted_labels = genel_fold_basarisi(fold_total,fold_correct,fold_true,fold_predicted)\n",
    "        print(\"fold 3 analizi bitti\")\n",
    "\n",
    "        dataset_dir = dataset_path + '/fold_4'\n",
    "        dataloaders, dataset_sizes = prepare_data(dataset_dir)\n",
    "        model_ft,criterion,optimizer_ft,exp_lr_scheduler=restart_model(opti,learning_rate_selection)\n",
    "        model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs,fold_num=4)\n",
    "        fold_total,fold_correct,fold_true,fold_predicted = conf_mat(model_ft,\"4\")\n",
    "        fold_accuracies,fold_true_labels,fold_predicted_labels = genel_fold_basarisi(fold_total,fold_correct,fold_true,fold_predicted)\n",
    "        print(\"fold 4 analizi bitti\")\n",
    "\n",
    "        dataset_dir = dataset_path + '/fold_5'\n",
    "        dataloaders, dataset_sizes = prepare_data(dataset_dir)\n",
    "        model_ft,criterion,optimizer_ft,exp_lr_scheduler=restart_model(opti,learning_rate_selection)\n",
    "        model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs,fold_num=5)\n",
    "        fold_total,fold_correct,fold_true,fold_predicted = conf_mat(model_ft,\"5\")\n",
    "        fold_accuracies,fold_true_labels,fold_predicted_labels = genel_fold_basarisi(fold_total,fold_correct,fold_true,fold_predicted)\n",
    "        print(\"fold 5 analizi bitti\")\n",
    "\n",
    "        fold_conf_mat(fold_true_labels,fold_predicted_labels,opti,learning_rate_selection,imageName,txtName,model_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a096edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcac41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7212cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
